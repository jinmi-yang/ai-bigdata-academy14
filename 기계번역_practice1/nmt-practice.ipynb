{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field, BucketIterator, interleave_keys\n",
    "from torchtext.datasets import TranslationDataset\n",
    "from torchtext.data import Example\n",
    "from mosestokenizer import *\n",
    "import torch\n",
    "\n",
    "from typing import Tuple\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import math\n",
    "import time\n",
    "\n",
    "# Author: WonKee Lee (POSTECH)\n",
    "# \"Neural Machine Translation by Jointly Learning to Align and Translate\" 논문의 model 재현 (Toy code)\n",
    "#  (https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html 를 참고하여 수정함.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### torchtext #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = '<s>'    # Start symbol\n",
    "EOS = '</s>'   # End symbol\n",
    "PAD = '<pad>'  # padding symbol\n",
    "\n",
    "# ex) 'I am a boy.' -> ['I', 'am', 'a', 'boy']\n",
    "tok_en = MosesTokenizer('en')\n",
    "tok_fr = MosesTokenizer('fr')\n",
    "\n",
    "# Field: Tensor로 표현할 데이터의 타입, 처리 프로세스 등을 정의하는 객체\n",
    "src = Field(sequential=True, #사전을 이용해서 인덱스를 할 거다\n",
    "            use_vocab=True,\n",
    "            pad_token=PAD,#배치 해줄거다\n",
    "            tokenize=tok_en,\n",
    "            lower=True, #전부 소문자로 바꾸겠다\n",
    "            batch_first=True) # if=True shape:[Batch, length] else shape=[length, Batch]\n",
    "\n",
    "tgt = Field(sequential=True,\n",
    "            use_vocab=True,\n",
    "            pad_token=PAD,\n",
    "            tokenize=tok_fr,\n",
    "            lower=True,\n",
    "            init_token=BOS, #가장 앞에는 bos를 붙이고\n",
    "            eos_token=EOS, #가장 끝에는 eos를 붙인다.\n",
    "            batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_f = '/home/piai/다운로드/ch4/practice1/data/data' #data.en 과 data.fr 이므로 data까지 정의\n",
    "\n",
    "# parallel data 각각 (en, de) 을 src Field 와 tgt Field에 정의된 형태로 처리.\n",
    "parallel_dataset = TranslationDataset(path=prefix_f, exts=('.en', '.fr'), \n",
    "                                      fields=[('src', src), ('tgt', tgt)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.datasets.translation.TranslationDataset object at 0x7fed28cee5b0>\n",
      "dict_items([('src', ['you', 'were', 'in', 'a', 'coma', '.']), ('tgt', ['tu', 'étais', 'dans', 'le', 'coma', '.'])])\n"
     ]
    }
   ],
   "source": [
    "print(parallel_dataset) \n",
    "\n",
    "print(parallel_dataset.examples[22222].__dict__.items()) # src 및 tgt 에 대한 samples 를 포함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['you', 'were', 'in', 'a', 'coma', '.']\n"
     ]
    }
   ],
   "source": [
    "print(parallel_dataset.examples[22222].src) # src 출력 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tu', 'étais', 'dans', 'le', 'coma', '.']\n"
     ]
    }
   ],
   "source": [
    "print(parallel_dataset.examples[22222].tgt) # tgt 출력 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 사전 구축 ########\n",
    "# src, tgt 필드에 사전 구축\n",
    "src.build_vocab(parallel_dataset, max_size=15000)\n",
    "tgt.build_vocab(parallel_dataset, max_size=15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['freqs', 'itos', 'unk_index', 'stoi', 'vectors'])\n",
      "\n",
      "     <unk> |   0\n",
      "     <pad> |   1\n",
      "         . |   2\n",
      "         i |   3\n",
      "       you |   4\n",
      "        to |   5\n",
      "       the |   6\n",
      "         ? |   7\n",
      "         a |   8\n",
      "   &apos;t |   9\n",
      "        is |  10\n",
      "        it |  11\n",
      "        he |  12\n",
      "      that |  13\n",
      "   &apos;s |  14\n",
      "        of |  15\n",
      "\n",
      "tgt vocab\n"
     ]
    }
   ],
   "source": [
    "# 사전 내용 \n",
    "print(src.vocab.__dict__.keys())\n",
    "print('')\n",
    "# stoi : string to index 의 약자\n",
    "for i, (k, v) in enumerate(src.vocab.stoi.items()):\n",
    "    print ('{:>10s} | {:>3d}'.format(k, v))\n",
    "    if i == 15 : break\n",
    "print('')\n",
    "print('tgt vocab')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pad는 1로 표시, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = parallel_dataset.split(split_ratio=0.95) # 0.95 = train / 0.05 = valid 데이터로 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch iterator 생성.\n",
    "# iterator 를 반복하며 batch (src, tgt) 가 생성 됨.\n",
    "BATCH_SIZE = 3\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator = BucketIterator.splits((train, valid), batch_size=BATCH_SIZE,\n",
    "                                                    sort_key=lambda x: interleave_keys(len(x.src), len(x.tgt)),\n",
    "                                                    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterator 는 Batch 객체 (Tensor) 를 출력해주며, \n",
    "# Batch.src / Batch.tgt 로 parallel data각각에 대해 접근가능.\n",
    "\n",
    "# 예시.\n",
    "Batch = next(iter(train_iterator)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 41,  43,  85, 743,   2,   1,   1,   1,   1],\n",
       "        [  4,  20,   9,  42,  46, 665,   4,  32,   2],\n",
       "        [ 24,  36, 290,  98,   2,   1,   1,   1,   1]], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# src 에 저장된 데이터 출력\n",
    "# Field에 정의된 형식으로 데이터 전처리 (indexing 포함.)\n",
    "# 가장 긴 문장을 기준으로, 그 보다 짧은 문장은 Padding idx(=1) 을 부여.\n",
    "Batch.src "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2,   64,   95,  103,  109,   29,  411,    4,    3,    1,    1,    1,\n",
       "            1,    1],\n",
       "        [   2,   18,   15,   96,    9,   14,  164,  314,   18,   74,   53, 1722,\n",
       "            4,    3],\n",
       "        [   2,   27,  107, 1432,   25,  602,    4,    3,    1,    1,    1,    1,\n",
       "            1,    1]], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Field에 정의된 형식으로 데이터 전처리 (indexing + bos + eos + pad 토큰 처리 됨.)\n",
    "Batch.tgt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Network 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Encoder 정의.\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, src_ntoken: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = dropout\n",
    "        self.src_ntoken = src_ntoken\n",
    "\n",
    "        self.embedding = nn.Embedding(src_ntoken, hidden_dim, \n",
    "                                      padding_idx=src.vocab.stoi['<pad>'])\n",
    "        \n",
    "        self.rnn = nn.GRU(hidden_dim, hidden_dim, bidirectional = True, \n",
    "                          batch_first=True) # batch_first = [B, L, dim]\n",
    "        #입력크기, 출력크기, bidirectional = \n",
    "        \n",
    "        # bidirectional hidden을 하나의 hidden size로 mapping해주기 위한 Linear\n",
    "        self.fc = nn.Linear(hidden_dim * 2, hidden_dim) \n",
    "        #encoder의 마지막 hidden이 decoder의 첫번째로 들어가야 하는데, 2hidden_dim이면 안들어가지니까, 하나로만!\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src = (Batch, Length) Tensor\n",
    "        embedded = self.dropout(self.embedding(src)) # shape = (Batch, Length, hidden_dim)\n",
    "\n",
    "        # outputs: [B, L, D*2], hidden: [2, B, D] -> [1, B, D] + [1, B, D]\n",
    "        # Note: Bidirection=False 인 경우:  outputs: [B, L, D], hidden: [1, B, D]\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "\n",
    "        last_hidden = self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)) # bidirection Dim(x2)을 projection --> [B, D]\n",
    "        hidden = torch.tanh(last_hidden).unsqueeze(0) # last bidirectional hidden (=Decoder init hidden) --> [1, B, D]\n",
    "\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Attention 모듈 정의 ###\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 enc_hid_dim: int,\n",
    "                 dec_hid_dim: int,\n",
    "                 attn_dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "\n",
    "        attn_in = (enc_hid_dim * 2) + dec_hid_dim # bidirectional hidden + dec_hidden\n",
    "        self.linear = nn.Linear(attn_in, attn_dim)\n",
    "        self.merge = nn.Linear(attn_dim, 1)\n",
    "\n",
    "    def forward(self, decoder_hidden, encoder_outputs):\n",
    "        # decoder_hiden = (Batch, 1, Dim) 길이가 1씩 들어오기 때문.\n",
    "        src_len = encoder_outputs.shape[1] \n",
    "        repeated_decoder_hidden = decoder_hidden.repeat(1, src_len, 1) # [B, src_len, D] -> 각각의 src단어와 연산해주기 위해 늘려준 결과.\n",
    "\n",
    "        # enc의 각 step의 hidden + decoder의 hidden 의 결과값 # [B, src_len, D*2] --> [B, src_len, D]\n",
    "        # tanh(W*h_dec  + U*h_enc) 수식 부분.\n",
    "        energy = torch.tanh(self.linear(torch.cat((repeated_decoder_hidden, encoder_outputs), dim=-1))) \n",
    "        # torch.cat 은 concat과 같음, decoder_hidden 과 encoder을 concat하고 차원 수 바꿈\n",
    "        # self.linear은 위에서 정의한 것처럼, weight를 넣어주는 것, torch.tanh하면 탄젠트 함수 씌어줌\n",
    "        score = self.merge(energy).squeeze(-1) # [B, src_len] 각 src 단어에 대한 점수 -> V^T tanh(W*h_dec  + U*h_enc) 부분\n",
    "        normalized_score = F.softmax(score, dim=1)  # softmax를 통해 확률분포값으로 변환\n",
    "        return  normalized_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Decoder 모듈 정의 ####\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, dec_ntoken: int, dropout: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim # Decoder RNN의 previous hidden\n",
    "        self.dropout = dropout\n",
    "        self.attention = Attention(enc_hid_dim=hidden_dim, \n",
    "                                   dec_hid_dim=hidden_dim, \n",
    "                                   attn_dim=hidden_dim) # attention layer\n",
    "        \n",
    "        self.dec_ntoken = dec_ntoken # tgt vocab_size\n",
    "\n",
    "        self.embedding = nn.Embedding(dec_ntoken, hidden_dim, \n",
    "                                      padding_idx=tgt.vocab.stoi['<pad>'])\n",
    "        \n",
    "        self.rnn = nn.GRU(hidden_dim, hidden_dim, batch_first=True) # bidirectinal=False 임.\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(self.hidden_dim*3, dec_ntoken) # Vocab 크기로 linear projection\n",
    "        self.sm = nn.LogSoftmax(dim=-1) # 확률 분포 값.\n",
    "\n",
    "    def _context_rep(self, dec_out, enc_outs):\n",
    "        scores = self.attention(dec_out, enc_outs) # score = [B, src_len]\n",
    "        scores = scores.unsqueeze(1) # [B, 1, src_len] -> weight value (softmax)\n",
    "\n",
    "        # scores: (batch, 1, src_len),  ecn_outs: (Batch, src_len, dim)\n",
    "        context_vector = torch.bmm(scores, enc_outs) # weighted average -> (batch, 1, dec_dim): encoder의 각 hidden의 weighted sum\n",
    "        #bmm 은 매트릭스 멀티플케이션 연산 약자\n",
    "        return context_vector\n",
    "\n",
    "    def forward(self, input, decoder_hidden, encoder_outputs):\n",
    "        dec_outs = []\n",
    "        embedded = self.dropout(self.embedding(input)) # (Batch, length, Dim)\n",
    "        \n",
    "        # (Batch, 1, dim)  (batch, 1, dim) , ....,\n",
    "        for emb_t in embedded.split(1, dim=1): # Batch 별 각 단어 (=각 time step) 에 대한 embedding 출력 \n",
    "            rnn_out, decoder_hidden = self.rnn(emb_t, decoder_hidden) # feed input with previous decoder hidden at each step\n",
    "\n",
    "            context = self._context_rep(rnn_out, encoder_outputs) # C_t vector\n",
    "            rnn_context = self.dropout(torch.cat([rnn_out, context], dim=2)) \n",
    "            dec_out = self.linear(rnn_context) # W(H + C) \n",
    "            dec_outs += [self.sm(dec_out)]\n",
    "        #밑은 지엽적인 부분\n",
    "        if len(dec_outs) > 1:\n",
    "            dec_outs = dec_outs[:-1] # trg = trg[:-1] # <E> 는 Decoder 입력으로 고려하지 않음.\n",
    "            dec_outs = torch.cat(dec_outs, dim=1) # convert list into tensor : [B, L, vocab]\n",
    "\n",
    "        else: # step-wise 로 decoding 하는 경우,\n",
    "            dec_outs = dec_outs[0] # [B=1, L=1, vocab]\n",
    "\n",
    "        return dec_outs, decoder_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Seq-to-Seq 모델 정의 ###\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,\n",
    "                 encoder: nn.Module,\n",
    "                 decoder: nn.Module,\n",
    "                 device: torch.device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        encoder_outputs, hidden = self.encoder(src) # encoder_outputs = (Batch, length, Dim * 2) , hidden = (Batch, Dim)\n",
    "        dec_out, _ = self.decoder(trg, hidden, encoder_outputs)\n",
    "        return dec_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#하이퍼파라미터\n",
    "\n",
    "INPUT_DIM = len(src.vocab)  # src 사전 크기\n",
    "OUTPUT_DIM = len(tgt.vocab) # tgt 사전 크기\n",
    "HID_DIM = 128 # rnn, embedding, 등. 모든 hidden 크기를 해당 값으로 통일함. (실습의 용이성을 위함.)\n",
    "D_OUT = 0.1 # Dropout  확률\n",
    "BATCH_SIZE = 26 #(gpu 메모리에 맞춰서)\n",
    "\n",
    "train_iterator, valid_iterator = BucketIterator.splits((train, valid), batch_size=BATCH_SIZE,\n",
    "                                                    sort_key=lambda x: interleave_keys(len(x.src), len(x.tgt)),\n",
    "                                                    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 및 디코더 생성\n",
    "# Seq2Seq 모델 생성\n",
    "encoder = Encoder(HID_DIM, INPUT_DIM, D_OUT)\n",
    "decoder = Decoder(HID_DIM, OUTPUT_DIM, D_OUT)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m: nn.Module):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "\n",
    "\n",
    "model.apply(init_weights) # 모델 파라미터 초기화\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005) # Optimizer 설정\n",
    "criterion = nn.NLLLoss(ignore_index=tgt.vocab.stoi['<pad>'], reduction='mean') # LOSS 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(13296, 128, padding_idx=1)\n",
      "    (rnn): GRU(128, 128, batch_first=True, bidirectional=True)\n",
      "    (fc): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (linear): Linear(in_features=384, out_features=128, bias=True)\n",
      "      (merge): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "    (embedding): Embedding(15004, 128, padding_idx=1)\n",
      "    (rnn): GRU(128, 128, batch_first=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (linear): Linear(in_features=384, out_features=15004, bias=True)\n",
      "    (sm): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "The model has 9,778,461 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# 모델 정보 및 파라미터 수 출력\n",
    "def count_parameters(model: nn.Module):\n",
    "    print(model)\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모델 학습 함수 ###\n",
    "def train(model, iterator, optimize, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch.src\n",
    "        tgt = batch.tgt\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, tgt) # [batch, length, vocab_size]\n",
    "        output = output.view(-1, output.size(-1)) # flatten --> (batch * length, vocab_size)\n",
    "\n",
    "        tgt = tgt.unsqueeze(-1)[:,1:,:].squeeze(-1).contiguous() # 정답에는 <S>가 포함되지 않음으로, 이를 삭제\n",
    "        tgt = tgt.view(-1) # flatten = (batch * length)\n",
    "\n",
    "        loss = criterion(output, tgt) # tgt 이 내부적으로 one_hot으로 변환됨 --> (batch * length, vocab_size)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        if(((i+1) % int(len(iterator)*0.2)) == 0):\n",
    "            num_complete = batch.batch_size * (i+1)\n",
    "            total_size = batch.batch_size * int(len(iterator))\n",
    "            ratio = num_complete/total_size * 100\n",
    "            print('| Current Epoch:  {:>4d} / {:<5d} ({:2d}%) | Train Loss: {:3.3f}'.\n",
    "                  format(num_complete, batch.batch_size * int(len(iterator)), round(ratio), loss.item())\n",
    "                  )\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 모델 평가 함수 ###\n",
    "def evaluate(model: nn.Module, iterator: BucketIterator,\n",
    "             criterion: nn.Module):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, batch in enumerate(iterator):\n",
    "            src = batch.src\n",
    "            tgt = batch.tgt\n",
    "\n",
    "            output = model(src, tgt)\n",
    "            output = output.view(-1, output.size(-1)) # flatten (batch * length, vocab_size)\n",
    "\n",
    "            tgt = tgt.unsqueeze(-1)[:,1:,:].squeeze(-1).contiguous() # remove <S> placed at first from targets\n",
    "            tgt = tgt.view(-1) # flatten target with shape = (batch * length)\n",
    "            loss = criterion(output, tgt)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 시간 카운트를 위한 함수 #\n",
    "def epoch_time(start_time: int, end_time: int):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Current Epoch:  25792 / 129064 (20%) | Train Loss: 4.400\n",
      "| Current Epoch:  51584 / 129064 (40%) | Train Loss: 3.761\n",
      "| Current Epoch:  77376 / 129064 (60%) | Train Loss: 3.259\n",
      "| Current Epoch:  103168 / 129064 (80%) | Train Loss: 3.243\n",
      "| Current Epoch:  128960 / 129064 (100%) | Train Loss: 3.010\n",
      "=================================================================\n",
      "Epoch: 01 | Time: 4m 55s\n",
      "\tTrain Loss: 3.877 | Train PPL:  48.277\n",
      "\t Val. Loss: 2.869 |  Val. PPL:  17.622\n",
      "=================================================================\n",
      "| Current Epoch:  25792 / 129064 (20%) | Train Loss: 2.452\n",
      "| Current Epoch:  51584 / 129064 (40%) | Train Loss: 2.373\n",
      "| Current Epoch:  77376 / 129064 (60%) | Train Loss: 2.130\n",
      "| Current Epoch:  103168 / 129064 (80%) | Train Loss: 2.370\n",
      "| Current Epoch:  128960 / 129064 (100%) | Train Loss: 2.219\n",
      "=================================================================\n",
      "Epoch: 02 | Time: 4m 50s\n",
      "\tTrain Loss: 2.387 | Train PPL:  10.878\n",
      "\t Val. Loss: 1.961 |  Val. PPL:   7.104\n",
      "=================================================================\n",
      "| Current Epoch:  25792 / 129064 (20%) | Train Loss: 1.579\n",
      "| Current Epoch:  51584 / 129064 (40%) | Train Loss: 1.877\n",
      "| Current Epoch:  77376 / 129064 (60%) | Train Loss: 1.763\n",
      "| Current Epoch:  103168 / 129064 (80%) | Train Loss: 1.755\n",
      "| Current Epoch:  59520 / 59568 (100%) | Train Loss: 1.397\n",
      "=================================================================\n",
      "Epoch: 03 | Time: 4m 48s\n",
      "\tTrain Loss: 1.725 | Train PPL:   5.610\n",
      "\t Val. Loss: 1.583 |  Val. PPL:   4.868\n",
      "=================================================================\n",
      "| Current Epoch:  25792 / 129064 (20%) | Train Loss: 1.613\n",
      "| Current Epoch:  51584 / 129064 (40%) | Train Loss: 1.208\n",
      "| Current Epoch:  77376 / 129064 (60%) | Train Loss: 1.292\n",
      "| Current Epoch:  103168 / 129064 (80%) | Train Loss: 1.227\n",
      "| Current Epoch:  128960 / 129064 (100%) | Train Loss: 1.251\n",
      "=================================================================\n",
      "Epoch: 04 | Time: 4m 32s\n",
      "\tTrain Loss: 1.392 | Train PPL:   4.021\n",
      "\t Val. Loss: 1.403 |  Val. PPL:   4.068\n",
      "=================================================================\n",
      "| Current Epoch:  25792 / 129064 (20%) | Train Loss: 1.011\n",
      "| Current Epoch:  51584 / 129064 (40%) | Train Loss: 0.989\n",
      "| Current Epoch:  77376 / 129064 (60%) | Train Loss: 1.027\n",
      "| Current Epoch:  103168 / 129064 (80%) | Train Loss: 1.447\n",
      "| Current Epoch:  128960 / 129064 (100%) | Train Loss: 1.050\n",
      "=================================================================\n",
      "Epoch: 05 | Time: 4m 52s\n",
      "\tTrain Loss: 1.189 | Train PPL:   3.282\n",
      "\t Val. Loss: 1.277 |  Val. PPL:   3.588\n",
      "=================================================================\n",
      "| Current Epoch:  25792 / 129064 (20%) | Train Loss: 0.851\n",
      "| Current Epoch:  51584 / 129064 (40%) | Train Loss: 1.177\n",
      "| Current Epoch:  77376 / 129064 (60%) | Train Loss: 1.094\n",
      "| Current Epoch:  103168 / 129064 (80%) | Train Loss: 1.146\n",
      "| Current Epoch:  128960 / 129064 (100%) | Train Loss: 0.938\n",
      "=================================================================\n",
      "Epoch: 06 | Time: 4m 53s\n",
      "\tTrain Loss: 1.052 | Train PPL:   2.864\n",
      "\t Val. Loss: 1.205 |  Val. PPL:   3.337\n",
      "=================================================================\n",
      "| Current Epoch:  25792 / 129064 (20%) | Train Loss: 0.756\n",
      "| Current Epoch:  51584 / 129064 (40%) | Train Loss: 0.825\n",
      "| Current Epoch:  77376 / 129064 (60%) | Train Loss: 1.152\n",
      "| Current Epoch:  103168 / 129064 (80%) | Train Loss: 0.806\n",
      "| Current Epoch:  128960 / 129064 (100%) | Train Loss: 0.890\n",
      "=================================================================\n",
      "Epoch: 07 | Time: 4m 51s\n",
      "\tTrain Loss: 0.954 | Train PPL:   2.596\n",
      "\t Val. Loss: 1.157 |  Val. PPL:   3.181\n",
      "=================================================================\n",
      "| Current Epoch:  25792 / 129064 (20%) | Train Loss: 0.749\n",
      "| Current Epoch:  51584 / 129064 (40%) | Train Loss: 0.857\n",
      "| Current Epoch:  77376 / 129064 (60%) | Train Loss: 0.807\n",
      "| Current Epoch:  103168 / 129064 (80%) | Train Loss: 1.011\n",
      "| Current Epoch:  128960 / 129064 (100%) | Train Loss: 0.827\n",
      "=================================================================\n",
      "Epoch: 08 | Time: 4m 46s\n",
      "\tTrain Loss: 0.876 | Train PPL:   2.402\n",
      "\t Val. Loss: 1.125 |  Val. PPL:   3.080\n",
      "=================================================================\n",
      "| Current Epoch:  25792 / 129064 (20%) | Train Loss: 0.766\n",
      "| Current Epoch:  51584 / 129064 (40%) | Train Loss: 0.673\n",
      "| Current Epoch:  77376 / 129064 (60%) | Train Loss: 0.740\n",
      "| Current Epoch:  103168 / 129064 (80%) | Train Loss: 1.043\n",
      "| Current Epoch:  128960 / 129064 (100%) | Train Loss: 0.917\n",
      "=================================================================\n",
      "Epoch: 09 | Time: 4m 44s\n",
      "\tTrain Loss: 0.817 | Train PPL:   2.263\n",
      "\t Val. Loss: 1.113 |  Val. PPL:   3.043\n",
      "=================================================================\n",
      "| Current Epoch:  25792 / 129064 (20%) | Train Loss: 0.469\n",
      "| Current Epoch:  51584 / 129064 (40%) | Train Loss: 0.791\n",
      "| Current Epoch:  77376 / 129064 (60%) | Train Loss: 0.992\n",
      "| Current Epoch:  103168 / 129064 (80%) | Train Loss: 0.968\n",
      "| Current Epoch:  128960 / 129064 (100%) | Train Loss: 0.642\n",
      "=================================================================\n",
      "Epoch: 10 | Time: 4m 52s\n",
      "\tTrain Loss: 0.768 | Train PPL:   2.155\n",
      "\t Val. Loss: 1.095 |  Val. PPL:   2.989\n",
      "=================================================================\n",
      "| Current Epoch:  25792 / 129064 (20%) | Train Loss: 0.584\n",
      "| Current Epoch:  51584 / 129064 (40%) | Train Loss: 0.763\n",
      "| Current Epoch:  77376 / 129064 (60%) | Train Loss: 0.704\n",
      "| Current Epoch:  103168 / 129064 (80%) | Train Loss: 0.735\n",
      "| Current Epoch:  128960 / 129064 (100%) | Train Loss: 0.714\n",
      "=================================================================\n",
      "Epoch: 11 | Time: 4m 39s\n",
      "\tTrain Loss: 0.727 | Train PPL:   2.069\n",
      "\t Val. Loss: 1.087 |  Val. PPL:   2.964\n",
      "=================================================================\n",
      "| Current Epoch:  25792 / 129064 (20%) | Train Loss: 0.607\n",
      "| Current Epoch:  51584 / 129064 (40%) | Train Loss: 0.624\n",
      "| Current Epoch:  77376 / 129064 (60%) | Train Loss: 0.586\n",
      "| Current Epoch:  103168 / 129064 (80%) | Train Loss: 0.730\n",
      "| Current Epoch:  128960 / 129064 (100%) | Train Loss: 0.712\n",
      "=================================================================\n",
      "Epoch: 12 | Time: 4m 52s\n",
      "\tTrain Loss: 0.693 | Train PPL:   1.999\n",
      "\t Val. Loss: 1.086 |  Val. PPL:   2.963\n",
      "=================================================================\n",
      "| Current Epoch:  25792 / 129064 (20%) | Train Loss: 0.553\n",
      "| Current Epoch:  51584 / 129064 (40%) | Train Loss: 1.021\n",
      "| Current Epoch:  77376 / 129064 (60%) | Train Loss: 0.623\n",
      "| Current Epoch:  103168 / 129064 (80%) | Train Loss: 0.808\n",
      "| Current Epoch:  128960 / 129064 (100%) | Train Loss: 0.673\n",
      "=================================================================\n",
      "Epoch: 13 | Time: 4m 12s\n",
      "\tTrain Loss: 0.663 | Train PPL:   1.940\n",
      "\t Val. Loss: 1.081 |  Val. PPL:   2.947\n",
      "=================================================================\n",
      "| Current Epoch:  25792 / 129064 (20%) | Train Loss: 0.610\n",
      "| Current Epoch:  51584 / 129064 (40%) | Train Loss: 0.576\n",
      "| Current Epoch:  77376 / 129064 (60%) | Train Loss: 0.731\n",
      "| Current Epoch:  103168 / 129064 (80%) | Train Loss: 0.633\n",
      "| Current Epoch:  128960 / 129064 (100%) | Train Loss: 0.525\n",
      "=================================================================\n",
      "Epoch: 14 | Time: 4m 45s\n",
      "\tTrain Loss: 0.638 | Train PPL:   1.892\n",
      "\t Val. Loss: 1.080 |  Val. PPL:   2.945\n",
      "=================================================================\n",
      "| Current Epoch:  25792 / 129064 (20%) | Train Loss: 0.549\n",
      "| Current Epoch:  51584 / 129064 (40%) | Train Loss: 0.626\n",
      "| Current Epoch:  77376 / 129064 (60%) | Train Loss: 0.696\n",
      "| Current Epoch:  103168 / 129064 (80%) | Train Loss: 0.629\n",
      "| Current Epoch:  128960 / 129064 (100%) | Train Loss: 0.640\n",
      "=================================================================\n",
      "Epoch: 15 | Time: 4m 33s\n",
      "\tTrain Loss: 0.615 | Train PPL:   1.851\n",
      "\t Val. Loss: 1.079 |  Val. PPL:   2.941\n",
      "=================================================================\n",
      "model saving..\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 15 # 최대 epoch 크기\n",
    "CLIP = 0.2 # weight cliping \n",
    "isTrain = True # True 인 경우 아래 학습 코드 실행, False인 경우 저장된 model 로드만 수행.\n",
    "\n",
    "if isTrain:\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "        valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        print('='*65)\n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "        print('='*65)\n",
    "\n",
    "    with open('NMT.pt', 'wb') as f:\n",
    "        print(\"model saving..\")\n",
    "        torch.save(model, f)\n",
    "\n",
    "else:\n",
    "    with open('NMT.pt', 'rb') as f:\n",
    "        model = torch.load(f).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Greedy decoding \n",
    "def greedy_decoding(model, input, fields, maxLen=20):\n",
    "    src_field = [('src', fields[0])]\n",
    "    tgt_field = fields[1]\n",
    "\n",
    "    ex = Example.fromlist([input], src_field) # field에 정의된 내용으로 전처리 (tokenizing) 수행\n",
    "    src_tensor = src.numericalize([ex.src], device) # torch.Tensor로 치환, indexing, bos, eos 등의 처리과정도 함께 적용됨.\n",
    "    tgt_tensor = torch.tensor([[tgt_field.vocab.stoi['<s>']]], device=device) # Decoder 초기 입력 \n",
    "    model.eval()\n",
    "\n",
    "    dec_result = []\n",
    "    with torch.no_grad():\n",
    "        enc_out, hidden = model.encoder(src_tensor)\n",
    "        for i in range(maxLen):\n",
    "            # Step1: tgt_tensor (입력) 과 인코더의 출력을 이용하여 디코더 결과 출력\n",
    "            # Do someting about Step1 here..\n",
    "            # --> dec_step, hidden = model.decoder(....)\n",
    "            dec_out, _ = model.deoder(tgt_tensor, hidden, enc_out)\n",
    "            \n",
    "            # Step2: 디코더의 출력결과 (확룰분포) 에서 Top1 에 해당하는 word Index 추출\n",
    "            # Do someting about Step2 here..\n",
    "            # use torch.topk(..) \n",
    "            _, top_idx = torch.topk(...)\n",
    "            # Step3: \n",
    "            # if: 출력된 word Index == EOS 인 경우 디코딩 중지 (break).\n",
    "            # else: 출력된 word Index를 저장하고, 다음 step의 디코더 입력 (tgt_tensor)으로 전달\n",
    "            if tgt_field.vocab.itos[top_idx] == '</s>':\n",
    "                break\n",
    "            else :\n",
    "                dec_result.append(top_idx.item())\n",
    "                tgt_tensor = top_idx.view(1,1)\n",
    "        \n",
    "    dec_result = [tgt_field.vocab.itos[w] for w in dec_result] # Word index를 단어로 치환\n",
    "    return dec_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy decoding 수행\n",
    "\n",
    "input_sent = input('Enter a english sentence:  ')\n",
    "output = greedy_decoding(model, input_sent, fields=(src, tgt))\n",
    "output = MosesDetokenizer('fr')(output)\n",
    "print('> ', input_sent)\n",
    "print('< ', output)\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIBD",
   "language": "python",
   "name": "aibd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
