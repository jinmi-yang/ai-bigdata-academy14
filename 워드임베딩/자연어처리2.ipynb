{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#사전에 등록된 단어에 대한 embedding (사전에 단어가 이 정도라고 가정)\n",
    "\n",
    "vocab = {\n",
    "    '청년': 0,\n",
    "    'AI': 1,\n",
    "    'BIGDATA': 2,\n",
    "    'NLP': 3,\n",
    "    '인공지능': 4,\n",
    "    'embedding': 5,\n",
    "    '실습': 6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(7, 10)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0872, -1.4790, -0.9969, -0.2104,  0.0785, -0.4328, -0.5781, -1.7808,\n",
      "          0.3627, -0.6589],\n",
      "        [-0.9827,  1.2339,  0.5383,  0.5564,  0.3178, -0.0818,  1.3327, -1.6270,\n",
      "          0.0414,  1.6149],\n",
      "        [ 0.2427,  0.1135,  0.5664,  0.0369,  0.0078, -1.2350,  0.5184, -1.8191,\n",
      "          0.3577, -0.6100],\n",
      "        [ 0.2125,  0.9560,  0.2885,  0.2823, -0.3762,  2.1083, -0.6447,  0.6969,\n",
      "         -0.8811,  0.5818],\n",
      "        [ 0.0640, -0.2242, -0.1968, -0.2314, -0.3154,  0.9090,  0.0802, -1.9778,\n",
      "         -0.5906,  0.3587],\n",
      "        [-1.7746,  1.0777, -0.9783,  0.6325, -0.9003, -2.4077, -0.7904, -0.7877,\n",
      "          1.4719, -1.3309],\n",
      "        [ 1.2571, -1.3857,  0.8845,  0.2980, -0.0672,  0.0432, -0.8921,  0.6939,\n",
      "          0.0297,  0.9928]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#weight matrix 설정\n",
    "\n",
    "dim=10\n",
    "emb_mtx = torch.nn.Embedding(len(vocab), dim) #행은 단어의 개수(사전의 크기), 열은 하이퍼파라미터로 설정\n",
    "\n",
    "print(emb_mtx)\n",
    "print(emb_mtx.weight) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0872, -1.4790, -0.9969, -0.2104,  0.0785, -0.4328, -0.5781, -1.7808,\n",
       "          0.3627, -0.6589]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Practice 1 ##\n",
    "\n",
    "# print word embedding of '청년'\n",
    "idx = torch.tensor([vocab['청년']], dtype=torch.long) \n",
    "#dtype은 long 타입을 int 타입을 넣거나 하면 되는데, 유효 자리 수가 더 길어지냐 마냐의 정도 차이 (그냥 보통 long을 넣는 게 안전하다)\n",
    "#torch.tensor에 넣을 때는 리스트로 넣어줘야 하기 때문에, 무조건 list로 묶어서 넣는다.\n",
    "# practice: make index tensor for '청년'\n",
    "print(idx)\n",
    "emb_mtx(idx) # feed index tensor to emb_mtx\n",
    "#torch.tensor로 그냥 idx 자체를 weight matrix에 넣으면, one-hot 할 필요 없이 자동으로 구해줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2]) torch.Size([3])\n",
      "\n",
      "tensor([[ 0.0872, -1.4790, -0.9969, -0.2104,  0.0785, -0.4328, -0.5781, -1.7808,\n",
      "          0.3627, -0.6589],\n",
      "        [-0.9827,  1.2339,  0.5383,  0.5564,  0.3178, -0.0818,  1.3327, -1.6270,\n",
      "          0.0414,  1.6149],\n",
      "        [ 0.2427,  0.1135,  0.5664,  0.0369,  0.0078, -1.2350,  0.5184, -1.8191,\n",
      "          0.3577, -0.6100]], grad_fn=<EmbeddingBackward>) torch.Size([3, 10])\n"
     ]
    }
   ],
   "source": [
    "## Practice 2 ##\n",
    "#문장의 형태에서 embedding\n",
    "\n",
    "# print word embeddings for a given sentence: \"청년 AI BIGDATA\"\n",
    "sent = ['청년', 'AI', 'BIGDATA'] #문장을 띄어쓰기 기준으로 분리\n",
    "idxs = [] #빈 리스트 > torch.tensor할 리스트를 만들어 둠\n",
    "\n",
    "#넣어준 데이터 sent에서 vocab[word](사전에서의 개수)를 인덱스 리스트에 추가\n",
    "for word in sent:\n",
    "    idx = vocab[word] # practice: convert word into index\n",
    "    idxs.append(idx) # append idx to idxs\n",
    "\n",
    "#리스트라서 그냥 적어도 되고, long으로 타입 받아본다.\n",
    "idxs = torch.tensor(idxs, dtype=torch.long) # practice: convert index list to tensor\n",
    "print(idxs, idxs.size())\n",
    "print('')\n",
    "\n",
    "emb = emb_mtx(idxs) #weight matrix 거친 값\n",
    "# practice: convert idxs into word embedding\n",
    "print(emb, emb.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 5, 6]]) torch.Size([2, 3])\n",
      "\n",
      "tensor([[[ 0.0872, -1.4790, -0.9969, -0.2104,  0.0785, -0.4328, -0.5781,\n",
      "          -1.7808,  0.3627, -0.6589],\n",
      "         [-0.9827,  1.2339,  0.5383,  0.5564,  0.3178, -0.0818,  1.3327,\n",
      "          -1.6270,  0.0414,  1.6149],\n",
      "         [ 0.2427,  0.1135,  0.5664,  0.0369,  0.0078, -1.2350,  0.5184,\n",
      "          -1.8191,  0.3577, -0.6100]],\n",
      "\n",
      "        [[ 0.2125,  0.9560,  0.2885,  0.2823, -0.3762,  2.1083, -0.6447,\n",
      "           0.6969, -0.8811,  0.5818],\n",
      "         [-1.7746,  1.0777, -0.9783,  0.6325, -0.9003, -2.4077, -0.7904,\n",
      "          -0.7877,  1.4719, -1.3309],\n",
      "         [ 1.2571, -1.3857,  0.8845,  0.2980, -0.0672,  0.0432, -0.8921,\n",
      "           0.6939,  0.0297,  0.9928]]], grad_fn=<EmbeddingBackward>) torch.Size([2, 3, 10])\n"
     ]
    }
   ],
   "source": [
    "## Practice 03 ##\n",
    "#batch(2개 이상의 문장)의 embedding\n",
    "# construct batch level word embeddings: [\"청년 AI BIGDATA\", \"NLP embedding 실습\"]\n",
    "sents = [['청년', 'AI', 'BIGDATA'],\n",
    "         ['NLP', 'embedding', '실습']]\n",
    "#sents의 사이즈는 행: 문장 개수, 열: 문장의 길이\n",
    "batch_idxs = [] #빈리스트 생성 > torch.tensor\n",
    "\n",
    "for sent in sents:\n",
    "    idxs = []\n",
    "    for word in sent:\n",
    "        idx = vocab[word] # practice: convert word into index\n",
    "        idxs.append(idx)  # append idx to idxs\n",
    "    batch_idxs.append(idxs) # append idxs to batch idx\n",
    "\n",
    "batch_idxs = torch.tensor(batch_idxs, dtype = torch.long) # practice: convert batch idx to tensor\n",
    "print(batch_idxs, batch_idxs.size()) \n",
    "print('')\n",
    "\n",
    "emb = emb_mtx(batch_idxs) # practice: convert idxs into word embedding\n",
    "print(emb, emb.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIBD",
   "language": "python",
   "name": "aibd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
