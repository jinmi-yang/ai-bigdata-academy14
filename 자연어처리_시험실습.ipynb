{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/piai/anaconda3/lib/python3.8/site-packages (3.5)\n",
      "Requirement already satisfied: click in /home/piai/anaconda3/lib/python3.8/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in /home/piai/anaconda3/lib/python3.8/site-packages (from nltk) (2020.10.15)\n",
      "Requirement already satisfied: tqdm in /home/piai/anaconda3/lib/python3.8/site-packages (from nltk) (4.50.2)\n",
      "Requirement already satisfied: joblib in /home/piai/anaconda3/lib/python3.8/site-packages (from nltk) (0.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.utils.rnn as rnn\n",
    "import statistics\n",
    "import nltk # 없으시면 설치하세요: pip install nltk\n",
    "import random\n",
    "import collections\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gpu 디바이스 설정\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dictionary class 선언 (사전 만드는 부분)\n",
    "class Dictionary(object):\n",
    "    def __init__(self, dataset, size):\n",
    "        ## init vocab ##\n",
    "        self.word2idx = {'<pad>':0, '<sos>': 1, '<eos>': 2, '<unk>': 3} # 사전 \n",
    "        self.idx2word = ['<pad>', '<sos>', '<eos>', '<unk>'] # inverted dictionary\n",
    "        #sos는 시작, eos는 끝, unk는 사전에 없는 단어가 등장할 경우 unk에 대한 인덱스 부여\n",
    "        #self.word2idx['unk'] -> 3\n",
    "        #self.idx2word[3] -> unk\n",
    "\n",
    "        self.build_dict(dataset, size)\n",
    "        #dataset은 데이터, size는 사전 크기(1만으로 할 지, 2만으로 할 지, 하이퍼 파라미터 (너무 높아도 문제, 학습해야 할 부분이 많아서) )\n",
    "\n",
    "    def __call__(self, word):\n",
    "        return self.word2idx.get(word, self.word2idx['<unk>']) # if word does not exist in vocab then return unk idx\n",
    "    #키가 있으면 키에 대한 값을 출력, 없으면 unk의 값을 출력\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def build_dict(self, dataset, dict_size): #밑에서 가져온 문장들을 통해 사전 만드는 법 (가장 많이 등장한 단어 순서로 1,2,3..)\n",
    "        ## Practice ##\n",
    "        \"\"\"Tokenize a text file.\"\"\"\n",
    "        total_words = (word for sent in dataset for word in sent) # store all words into tuple (단어 등장 빈도 수를 알아내고)\n",
    "        word_freq = collections.Counter(total_words)# count the number of each word: ex) ('The': 10000, 'a': 5555, ...) (그걸 튜플로 묶어냄)\n",
    "        vocab = sorted(word_freq.keys(), key=lambda word: (-word_freq[word], word)) # sort by frequency\n",
    "        #\n",
    "        vocab = vocab[:dict_size] # truncate\n",
    "        for word in vocab:\n",
    "            self.add_word(word)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Brown dataset Preprocessing (NLTK)\n",
    "def brown_dataset(min=5, max=30): #문장 길이가 min~max 사이인 것만 받고, 모든 문장을 소문자로 치환\n",
    "    nltk.download('brown')\n",
    "\n",
    "    # get sentences with the length between min and max\n",
    "    # convert all words into lower-case\n",
    "    all_seq = [[token.lower() for token in seq] for seq in nltk.corpus.brown.sents() \n",
    "               if min <= len(seq) <= max]\n",
    "\n",
    "    random.shuffle(all_seq) # shuffle\n",
    "    return all_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/piai/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43450\n",
      "['we', 'followed', 'the', 'asphalt', 'road', 'for', 'a', 'few', 'miles', 'and', 'then', 'swung', 'off', 'onto', 'a', 'smaller', 'road', 'which', 'was', 'nothing', 'more', 'than', 'two', 'tire', 'marks', 'on', 'the', 'earth', '.']\n",
      "['press', 'scored', 'handle', 'ends', 'firmly', 'in', 'place', 'using', 'dowel', 'to', 'reinforce', 'container', 'while', 'pressing', ';', ';']\n",
      "['you', 'know', 'that', 'i', 'could', 'hold', 'right', 'here', 'in', 'my', 'hand', 'the', 'little', 'chunk', 'of', 'uranium', 'metal', 'that', 'was', 'the', 'heart', 'of', 'the', 'bomb', 'that', 'dropped', 'on', 'hiroshima', '.']\n"
     ]
    }
   ],
   "source": [
    "## Download Brown dataset\n",
    "dataset = brown_dataset()\n",
    "print(len(dataset)) #문장이 몇 개인가(43450개는 사실 작은 데이터양, 100만개 단위가 되어도 모자란 경우 있을 수)\n",
    "## print some part\n",
    "print(dataset[0]) #한 문장씩 저장되어 있는 것을 알 수 있음\n",
    "print(dataset[1])\n",
    "print(dataset[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data handler class 선언\n",
    "class Corpus(object):\n",
    "    def __init__(self, dataset, device, dict_size=20000, train_ratio=0.97):\n",
    "        #43450개의 데이터 중 97%를 학습용으로 쓰고, 나머지 3%를 test로 사용하겠다\n",
    "        \n",
    "        train_size = int(len(dataset) * train_ratio)\n",
    "        self.device = device\n",
    "        self.dictionary = Dictionary(dataset, dict_size) #위의 딕셔너리 클래스\n",
    "        self.train = dataset[:train_size] # [0 ~ train_size]\n",
    "        self.valid = dataset[train_size:] # [train_size: len(dataset)]\n",
    "\n",
    "    def indexing(self, dat):\n",
    "        # dat = list(list)\n",
    "        \n",
    "        src_idxes = [] # 모델 입력\n",
    "        tgt_idxes = [] # 모델 정답\n",
    "        for sent in dat:\n",
    "            #torch.tensor은 리스트로 받아야 하므로,\n",
    "            src_idx = [self.dictionary('<sos>')] + [self.dictionary(word) for word in sent] #입력받은 애들을\n",
    "            tgt_idx = [self.dictionary(word) for word in sent] + [self.dictionary('<eos>')]\n",
    "            src_idxes.append(torch.tensor(src_idx).type(torch.int64)) \n",
    "            tgt_idxes.append(torch.tensor(tgt_idx).type(torch.int64))\n",
    "        \n",
    "        #batch_first를 true로 두면, B*L로 둔다. (즉, 칸이 모자란 짧은 문장에 batch해줌)\n",
    "        #view(-1): 미리 플랫하게 만들어둠, 나중에 flat하게 가야 하므로\n",
    "        src_idxes = rnn.pad_sequence(src_idxes, batch_first=True).to(self.device) # shape = [B, L]\n",
    "        tgt_idxes = rnn.pad_sequence(tgt_idxes, batch_first=True).to(self.device).view(-1) # flatten shape = [B * L]\n",
    "\n",
    "        return src_idxes, tgt_idxes\n",
    "\n",
    "    def batch_iter(self, batch_size, isTrain=True):\n",
    "        dat = self.train if isTrain else self.valid\n",
    "        if isTrain:\n",
    "            random.shuffle(dat)\n",
    "\n",
    "        for i in range(len(dat) // batch_size):\n",
    "            batch = dat[i * batch_size: (i+1) * batch_size]\n",
    "            src, tgt = self.indexing(batch)\n",
    "            yield {'src': src, 'tgt': tgt}\n",
    "            #트레이닝 할 때 출력함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus(dataset, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word:  <pad>      | index:     0 \n",
      "word:  <sos>      | index:     1 \n",
      "word:  <eos>      | index:     2 \n",
      "word:  <unk>      | index:     3 \n",
      "word:  the        | index:     4 \n",
      "word:  .          | index:     5 \n",
      "word:  ,          | index:     6 \n",
      "word:  of         | index:     7 \n",
      "word:  and        | index:     8 \n",
      "word:  to         | index:     9 \n",
      "word:  a          | index:    10 \n",
      "word:  in         | index:    11 \n",
      "word:  was        | index:    12 \n",
      "word:  he         | index:    13 \n",
      "word:  is         | index:    14 \n",
      "word:  ''         | index:    15 \n",
      "word:  ``         | index:    16 \n",
      "word:  it         | index:    17 \n",
      "word:  that       | index:    18 \n",
      "word:  for        | index:    19 \n",
      "word:  ;          | index:    20 \n"
     ]
    }
   ],
   "source": [
    "# Dictionary 확인\n",
    "for i, (key, val) in enumerate(corpus.dictionary.word2idx.items()):\n",
    "    print('word:  {:10s} | index: {:5d} '.format(key, val))\n",
    "    if i == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['press', 'scored', 'handle', 'ends', 'firmly', 'in', 'place', 'using', 'dowel', 'to', 'reinforce', 'container', 'while', 'pressing', ';', ';']]\n",
      "tensor([[    1,   807,  4764,  2048,  1395,  2490,    11,   174,   665, 17037,\n",
      "             9,  7694,  6083,   188,  3818,    20,    20]], device='cuda:0')\n",
      "tensor([  807,  4764,  2048,  1395,  2490,    11,   174,   665, 17037,     9,\n",
      "         7694,  6083,   188,  3818,    20,    20,     2], device='cuda:0')\n",
      "------------------------------------------------------------------------------------------\n",
      "[['we', 'followed', 'the', 'asphalt', 'road', 'for', 'a', 'few', 'miles', 'and', 'then', 'swung', 'off', 'onto', 'a', 'smaller', 'road', 'which', 'was', 'nothing', 'more', 'than', 'two', 'tire', 'marks', 'on', 'the', 'earth', '.'], ['press', 'scored', 'handle', 'ends', 'firmly', 'in', 'place', 'using', 'dowel', 'to', 'reinforce', 'container', 'while', 'pressing', ';', ';']]\n",
      "tensor([[    1,    48,   516,     4, 15767,   502,    19,    10,   173,   578,\n",
      "             8,    83,  1751,   142,  1478,    10,  1606,   502,    51,    12,\n",
      "           198,    60,    73,    87,  4803,  3437,    24,     4,   631,     5],\n",
      "        [    1,   807,  4764,  2048,  1395,  2490,    11,   174,   665, 17037,\n",
      "             9,  7694,  6083,   188,  3818,    20,    20,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]],\n",
      "       device='cuda:0')\n",
      "tensor([   48,   516,     4, 15767,   502,    19,    10,   173,   578,     8,\n",
      "           83,  1751,   142,  1478,    10,  1606,   502,    51,    12,   198,\n",
      "           60,    73,    87,  4803,  3437,    24,     4,   631,     5,     2,\n",
      "          807,  4764,  2048,  1395,  2490,    11,   174,   665, 17037,     9,\n",
      "         7694,  6083,   188,  3818,    20,    20,     2,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "## indexing 함수 결과 확인 (문장 하나만 입력했다고 했을 떄 확인)\n",
    "\n",
    "# case : 단일 문장 입력 시. \n",
    "sent = [dataset[1]]\n",
    "idx_src, idx_tgt = corpus.indexing(sent)\n",
    "\n",
    "print(sent)\n",
    "print(idx_src) # <SOS> index로 시작\n",
    "print(idx_tgt) # <EOS> index로 종료\n",
    "\n",
    "print('-' * 90)\n",
    "## case : 복수 문장 입력 시 (batching)\n",
    "batch = [dataset[0], dataset[1]]\n",
    "idx_src, idx_tgt = corpus.indexing(batch)\n",
    "\n",
    "print(batch)\n",
    "print(idx_src) # 가장 길이가 긴 문장 (dataset[0]) 보다 짧은 문장 (dataset[1]) 의 경우 남는 길이만큼 padding=0 삽입 확인.\n",
    "print(idx_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RNN Language model 선언\n",
    "\n",
    "# Define network\n",
    "class RNNModel(nn.Module):\n",
    "    #init 부분에서는 학습할 weight, layer등을 정의하는 곳\n",
    "    #forward 부분에서는 init에서 저장된 weight를 이용해서 연산을 정의하는 곳\n",
    "    \n",
    "    def __init__(self, ntoken, hidden_size, nlayers, dropout=0.1):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout) #그냥 드랍아웃\n",
    "        self.embeddings = nn.Embedding(ntoken, hidden_size, padding_idx=0) #(딕셔너리크기, 차원의 크기)\n",
    "        self.rnn = nn.LSTM(hidden_size, hidden_size, nlayers, dropout=dropout, batch_first=True) #(입력차원의 크기, 나왔을 때 차원크기)\n",
    "        #입력 했을 때 차원 크기이므로, self.embedding 했을 때 나오는 차원의 크기와 같아야 함. \n",
    "        #나왔을 때 차원 크기는 그냥 임의로 같게 해줌\n",
    "        \n",
    "        self.output_layer = nn.Linear(hidden_size, ntoken) # 입력 차원의 크기, 나왔을 때 차원의 크기\n",
    "        #나왔을 때 차원의 크기는 반드시 사전의 크기와 같아야 하고, 입력 차원은 앞의 nn.LSTM에서의 나올 때 차원의 크기와 같아야 함\n",
    "        self.sm = nn.LogSoftmax(dim=-1) # log확률값(다시 로그 씌우지 않아도 됨, 여기서 로그 계산 됨, 그냥 더하기만 하면 됨)\n",
    "\n",
    "        self.ntoken = ntoken\n",
    "        self.hidden_size = hidden_size\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self): #모델 웨이트 초기화 (안중요함)\n",
    "        initrange = 0.1\n",
    "        self.embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        self.output_layer.weight.data.uniform_(-initrange, initrange)\n",
    "        self.output_layer.bias.data.zero_()\n",
    "\n",
    "    def forward(self, input, hidden):#학습 할 때 호출되는 부분(모델 생성에 중요)\n",
    "        # shape(input) = [Batch, length]\n",
    "        emb = self.embeddings(input) # emb = (batch, length, dim) > 3차원화\n",
    "        #input한 word들을 넣고, 그걸 embedding 한 것을 밑에 넣으면, 앞에서 ht-n과 자신의 것을 합쳐서 연결해서 구하는 것을 자동으로 해줌\n",
    "        output, hidden = self.rnn(emb) # output = (batch. length. dim) #output은 ht-n,..,ht-1까지, 히든은 마지막 것에 대한 h\n",
    "        output = self.drop(output) #dropout \n",
    "        output = self.output_layer(output) # linear projection : hidden dim --> vocab size\n",
    "        #마지막 소프트맥스 레이어에 들어가기 전에는 voca (사전)사이즈로 맞춰줘야 한다.\n",
    "        output = output.view(-1, self.ntoken) # output = (batch * length, vocab_size)\n",
    "        output = self.sm(output)# softmax\n",
    "\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, bsz): #첫번째 애도 앞에서 받는 게 있어야 하니까, 앞에 아무것도 없는 벡터가 있다는 것 같은 걸 만들어 줌\n",
    "        weight = next(self.parameters()) # to set init tensor with the same torch.dtype and torch.device\n",
    "        return (weight.new_zeros(self.nlayers, bsz, self.hidden_size),\n",
    "                weight.new_zeros(self.nlayers, bsz, self.hidden_size)) #H1 과 셀에 대해서 제로 벡터를 받으려구 두 개가 있음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piai/anaconda3/envs/AIBD/lib/python3.8/site-packages/torch/nn/modules/rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters (설정할 하이퍼파라미터)\n",
    "batch_size = 60\n",
    "hidden_size = 256\n",
    "dropout = 0.2\n",
    "max_epoch = 30\n",
    "\n",
    "# build model\n",
    "ntokens = len(corpus.dictionary) #사전의 크기\n",
    "model = RNNModel(ntokens, hidden_size, 1, dropout).to(device) #RNN 클래스 호출\n",
    "isTrain=True # Flag variable\n",
    "#TRUE로 두어야 모델이 학습됨, FALSE로 둔다는 건 처음 학습 한 걸로 계속 돌아감.(보통 계속 학습할 이유가 없으니까)\n",
    "\n",
    "# set loss func and optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "criterion = nn.NLLLoss(ignore_index=0, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Training / Evaluation Parts #######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy (학습 중간중간 정확도 계산부분)\n",
    "def cal_acc(scores, target):\n",
    "    pred = scores.max(-1)[1]\n",
    "    non_pad = target.ne(0)\n",
    "    num_correct = pred.eq(target).masked_select(non_pad).sum().item() \n",
    "    num_non_pad = non_pad.sum().item()\n",
    "    return 100 * (num_correct / num_non_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train func.\n",
    "# corpus부분 실행, forward 실행 부분\n",
    "\n",
    "def train():\n",
    "    model.train() # Turn on training mode which enables dropout.\n",
    "    mean_loss = []\n",
    "    mean_acc = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch in corpus.batch_iter(batch_size):\n",
    "        hidden = model.init_hidden(batch_size) # zero vectors for init hidden\n",
    "        target = batch['tgt'] # flattened target \n",
    "        optimizer.zero_grad()\n",
    "        output, hidden = model(batch['src'], hidden) # output = flatten output = [Batch_size * Length, vocab_size]\n",
    "        #이 때, 아웃풋은 각각의 voca 분포로 나옴\n",
    "        # output shape = (batch * length, vocab_size)\n",
    "        # target shape = (batch * length)   --> (batch * length, vocab_size) 로 one-hot distribtuion으로 내부적으로 변환되어 비교 수행\n",
    "        loss = criterion(output, target) # compare between vocab_prob and answer_prob(one-hot converted)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        mean_loss.append(loss.item())\n",
    "        mean_acc.append(cal_acc(output, target))\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    mean_acc = statistics.mean(mean_acc)\n",
    "    mean_loss = statistics.mean(mean_loss)\n",
    "\n",
    "    return mean_loss, total_time, mean_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation func.\n",
    "# train과 동일하나, 여기서 학습이 일어나지 않으므로, 학습 부분들은 필요없다.\n",
    "def evaluate():\n",
    "    model.eval() # Turn off dropout\n",
    "    mean_loss = []\n",
    "    mean_acc = []\n",
    "\n",
    "    for batch in corpus.batch_iter(batch_size, isTrain=False):\n",
    "        with torch.no_grad():\n",
    "            hidden = model.init_hidden(batch_size)\n",
    "            target = batch['tgt']\n",
    "            output, hidden = model(batch['src'], hidden)\n",
    "            loss = criterion(output, target)\n",
    "            mean_loss.append(loss.item())\n",
    "            mean_acc.append(cal_acc(output, target))\n",
    "\n",
    "    mean_acc = statistics.mean(mean_acc)\n",
    "    mean_loss = statistics.mean(mean_loss)\n",
    "\n",
    "    return mean_loss, mean_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    2 | times 16.601 |  loss: 5.688 | accuracy: 20.46\n",
      "epoch    3 | times 17.045 |  loss: 5.065 | accuracy: 23.93\n",
      "epoch    4 | times 17.660 |  loss: 4.710 | accuracy: 25.59\n",
      "epoch    5 | times 17.654 |  loss: 4.396 | accuracy: 27.11\n",
      "epoch    6 | times 17.697 |  loss: 4.110 | accuracy: 28.81\n",
      "epoch    7 | times 16.810 |  loss: 3.861 | accuracy: 30.74\n",
      "epoch    8 | times 17.214 |  loss: 3.653 | accuracy: 32.66\n",
      "epoch    9 | times 15.238 |  loss: 3.483 | accuracy: 34.43\n",
      "epoch   10 | times 16.889 |  loss: 3.340 | accuracy: 36.02\n",
      "epoch   11 | times 16.581 |  loss: 3.226 | accuracy: 37.44\n",
      "============================================================\n",
      "Evaluation | loss: 5.899 | accuracy: 22.96\n",
      "============================================================\n",
      "epoch   12 | times 17.798 |  loss: 3.128 | accuracy: 38.62\n",
      "epoch   13 | times 17.332 |  loss: 3.050 | accuracy: 39.59\n",
      "epoch   14 | times 17.421 |  loss: 2.987 | accuracy: 40.37\n",
      "epoch   15 | times 17.409 |  loss: 2.930 | accuracy: 41.15\n",
      "epoch   16 | times 16.840 |  loss: 2.885 | accuracy: 41.69\n",
      "epoch   17 | times 16.833 |  loss: 2.849 | accuracy: 42.16\n",
      "epoch   18 | times 17.524 |  loss: 2.817 | accuracy: 42.58\n",
      "epoch   19 | times 17.474 |  loss: 2.789 | accuracy: 42.84\n",
      "epoch   20 | times 16.696 |  loss: 2.767 | accuracy: 43.20\n",
      "epoch   21 | times 17.711 |  loss: 2.747 | accuracy: 43.39\n",
      "============================================================\n",
      "Evaluation | loss: 6.689 | accuracy: 22.13\n",
      "============================================================\n",
      "epoch   22 | times 17.719 |  loss: 2.728 | accuracy: 43.69\n",
      "epoch   23 | times 17.829 |  loss: 2.718 | accuracy: 43.78\n",
      "epoch   24 | times 16.538 |  loss: 2.705 | accuracy: 43.91\n",
      "epoch   25 | times 15.737 |  loss: 2.692 | accuracy: 44.02\n",
      "epoch   26 | times 17.028 |  loss: 2.688 | accuracy: 44.05\n",
      "epoch   27 | times 17.612 |  loss: 2.678 | accuracy: 44.21\n",
      "epoch   28 | times 17.433 |  loss: 2.672 | accuracy: 44.22\n",
      "epoch   29 | times 16.843 |  loss: 2.671 | accuracy: 44.21\n",
      "epoch   30 | times 17.356 |  loss: 2.670 | accuracy: 44.19\n",
      "epoch   31 | times 16.946 |  loss: 2.662 | accuracy: 44.34\n",
      "============================================================\n",
      "Evaluation | loss: 7.134 | accuracy: 21.90\n",
      "============================================================\n",
      "save model at: ./model.pt\n"
     ]
    }
   ],
   "source": [
    "if isTrain: # set False if you don't need to train model\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(1, max_epoch+1):\n",
    "        loss, epoch_time, accuracy = train()\n",
    "        print('epoch {:4d} | times {:3.3f} |  loss: {:3.3f} | accuracy: {:3.2f}'.format(epoch+1, epoch_time, loss, accuracy))\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            loss, accuracy = evaluate()\n",
    "            print('=' * 60)\n",
    "            print('Evaluation | loss: {:3.3f} | accuracy: {:3.2f}'.format(loss, accuracy))\n",
    "            print('=' * 60)\n",
    "\n",
    "    with open('model.pt', 'wb') as f:\n",
    "        print('save model at: ./model.pt')\n",
    "        torch.save(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_sent_prob(sent):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # 1. 모델 입력 및 정답 문장에 대한 단어 indexing\n",
    "        idx_src, idx_tgt = corpus.indexing(sent)\n",
    "        # 2. initial hidden 생성\n",
    "        hidden = model.init_hidden(len(sent))\n",
    "        # 3. LM의 결과(확률분포) 생성\n",
    "        output, hidden = model(idx_src, hidden)\n",
    "        # 4. 모델 확률분포로부터 정답 단어의 각 index에 대한 Log 확률 값 추출.\n",
    "        log_prob = []\n",
    "        for i in range(len(output)) :\n",
    "            log_prob.append(output[i][idx_tgt[i]])\n",
    "        # 5. log 확률의 합.\n",
    "        sent_prob = sum(log_prob)\n",
    "        # 6. 결과 return (return type: float)\n",
    "        return sent_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model from: ./model.pt\n",
      "log prob of [the dog bark .]: -39.488\n",
      "log prob of [the cat bark .]: -53.549\n",
      "log prob of [boy am a i .]: -47.778\n",
      "log prob of [i am a boy .]: -20.623\n"
     ]
    }
   ],
   "source": [
    "# load saved model\n",
    "with open('./model.pt', 'rb') as f:\n",
    "    print('load model from: ./model.pt')\n",
    "    model = torch.load(f).to(device)\n",
    "\n",
    "    print('log prob of [the dog bark .]: {:3.3f}'.format(pred_sent_prob([['the', 'dog', 'bark', '.']])))\n",
    "    print('log prob of [the cat bark .]: {:3.3f}'.format(pred_sent_prob([['the', 'cat', 'bark', '.']])))\n",
    "\n",
    "    print('log prob of [boy am a i .]: {:3.3f}'.format(pred_sent_prob([['boy', 'am', 'a', 'i', '.']])))\n",
    "    print('log prob of [i am a boy .]: {:3.3f}'.format(pred_sent_prob([['i', 'am', 'a', 'boy', '.']])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_next_word(partial_sent, topN=3):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # 1. 모델 입력 및 정답 문장에 대한 단어 indexing\n",
    "        idx_src, idx_tgt = corpus.indexing(partial_sent)\n",
    "        # 2. initial hidden 생성\n",
    "        hidden = model.init_hidden(len(partial_sent))\n",
    "        # 3. LM의 결과(확률분포) 생성\n",
    "        output, hidden = model(idx_src, hidden)\n",
    "        # 4. topN에 해당하는 다음단어의 word index 추출 (Hint: torch.topk() 활용)\n",
    "        values, idx_word = output.topk(topN)\n",
    "        # 5. word index --> word 로 변환\n",
    "        word_list = []\n",
    "        for i in range(topN) :\n",
    "            idx_category= idx_word[0][i].item()\n",
    "            word_list.append(corpus.dictionary.idx2word[idx_category])\n",
    "       \n",
    "        # 6. topN word list 반환 (return type: list)\n",
    "        return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 next words for a partial sentence [the next word] is: \n",
      "===> ['the', '``', 'he']\n"
     ]
    }
   ],
   "source": [
    "partial_sent = [['the', 'next', 'word']]\n",
    "N=3\n",
    "candidates = pred_next_word(partial_sent, topN=N)\n",
    "\n",
    "# print \n",
    "partial_sent = ' '.join(partial_sent[0])\n",
    "print('Top {0} next words for a partial sentence [{1}] is: '.format(N, partial_sent))\n",
    "print('===>', candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIBD",
   "language": "python",
   "name": "aibd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
